{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam sms classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdDQ_jDx7Ech"
      },
      "source": [
        "# Spam SMS classification\n",
        "## Name: Soumyadeep Deb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZNzzFwnWulv"
      },
      "source": [
        "## Importing Libraries and downloading resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSSRXxNve-OB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd280b3-1844-4bb2-a911-491fb264c067"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Error loading WordnetLemmatizer: Package\n",
            "[nltk_data]     'WordnetLemmatizer' not found in index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0RqKiebW5Yn"
      },
      "source": [
        "## Getting some info on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "Ebt954bFipbo",
        "outputId": "c51a34a8-28c6-4486-a2a3-81bab670c95b"
      },
      "source": [
        "df= pd.read_csv('spam.csv',encoding='latin-1')\n",
        "df.head()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1  ... Unnamed: 4\n",
              "0   ham  ...        NaN\n",
              "1   ham  ...        NaN\n",
              "2  spam  ...        NaN\n",
              "3   ham  ...        NaN\n",
              "4   ham  ...        NaN\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQTNwIy4iy6L",
        "outputId": "0add16b2-1c6e-4b36-dbaf-d56f19159667"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   v1          5572 non-null   object\n",
            " 1   v2          5572 non-null   object\n",
            " 2   Unnamed: 2  50 non-null     object\n",
            " 3   Unnamed: 3  12 non-null     object\n",
            " 4   Unnamed: 4  6 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8QHiVzqljrD"
      },
      "source": [
        "### We can simply drop columns 2-4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "iPuZq_XOm4KG",
        "outputId": "b5f1d6c9-3053-45da-ee61-eaa7e1b72528"
      },
      "source": [
        "df = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
        "df.sample(5)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3090</th>\n",
              "      <td>spam</td>\n",
              "      <td>LORD OF THE RINGS:RETURN OF THE KING in store ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3984</th>\n",
              "      <td>ham</td>\n",
              "      <td>Whatever, juliana. Do whatever you want.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4658</th>\n",
              "      <td>ham</td>\n",
              "      <td>I cant pick the phone right now. Pls send a me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5315</th>\n",
              "      <td>ham</td>\n",
              "      <td>Hahaha..use your brain dear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>ham</td>\n",
              "      <td>No screaming means shouting..</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        v1                                                 v2\n",
              "3090  spam  LORD OF THE RINGS:RETURN OF THE KING in store ...\n",
              "3984   ham           Whatever, juliana. Do whatever you want.\n",
              "4658   ham  I cant pick the phone right now. Pls send a me...\n",
              "5315   ham                        Hahaha..use your brain dear\n",
              "1527   ham                      No screaming means shouting.."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jxAP4BnIVA"
      },
      "source": [
        "## Text Pre-processing\n",
        "## Steps:\n",
        "1. Remove Punctuations\n",
        "2. Lowercase each word\n",
        "3. Word Tokenization\n",
        "4. Remove stopwords eg: the, a, is etc.\n",
        "5. Lemmatize / Stem each word\n",
        "6. Build the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCz09KQiwEaj"
      },
      "source": [
        "vocab = []      # The complete list of processed words will stored here\n",
        "sentences = df['v2'].values     # Sentences\n",
        "y = df['v1']                    # Label column"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5niankHnBMb"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re  # For Regular expression\n",
        "\n",
        "tokenize = word_tokenize\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMd_F_TCxK4K"
      },
      "source": [
        "# the Preprossecing part\n",
        "for i in range(len(sentences)):              \n",
        "    sent = sentences[i]\n",
        "    sent = re.sub('[^a-zA-Z]',' ',sent)     # Remove punctuations\n",
        "    sent = sent.lower()                     # Lowercas each word\n",
        "    words = tokenize(sent)                  # Tokenize each word\n",
        "    # Now, Remove stopwords and Stem\n",
        "    words = [stemmer.stem(word) for word in words if not word in stopwords.words(['english','french','german','spanish'])]\n",
        "\n",
        "    # Reverting back to form a complete sentence of processed words\n",
        "    sent = ' '.join(words)\n",
        "\n",
        "    # At last append this to vocabulary\n",
        "    vocab.append(sent)\n",
        "\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EME3zJA96oUh",
        "outputId": "c441776f-0178-4d91-fbb7-11ab1393d067"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO5eQ9dfEP5e",
        "outputId": "ac4b620c-a479-4882-e095-4bc4bd71afd5"
      },
      "source": [
        "y.value_counts()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: v1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdz_kSt-DK1O",
        "outputId": "d2a90984-c75c-4ade-fb44-04a99b85e7b2"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y\n",
        "\n",
        "## ham - 0 , spam - 1 ~ Encoding of target variable"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb3uLCD08S3c"
      },
      "source": [
        "## Building the model\n",
        "### We use the function deploy_and_evaluate to test the bag of words and tf-idf models on it.\n",
        "### Multinomial Naive Bayes has been used here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URg6aEE98sMM"
      },
      "source": [
        "def deploy_and_evaluate(X,y,test_size=0.3,random_state=5):\n",
        "    # Train test split\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = test_size, random_state = random_state )\n",
        "\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score\n",
        "\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train,y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluating the model\n",
        "    print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
        "    print(\"\\nAccuracy Score:\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "    print(\"\\nPrecision Score:\",precision_score(y_test,y_pred)*100,\"%\")\n",
        "    print(\"\\nRecall Score:\",recall_score(y_test,y_pred)*100,\"%\")\n",
        "\n",
        "    return model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK2-Ne1CD5VI"
      },
      "source": [
        "## Now, let's form a Bag of Words model and test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKMQAd593Dtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf53cfa-ecf8-43d5-907b-c25666bfeb81"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bag_vectorizer = CountVectorizer()\n",
        "\n",
        "X_bag = bag_vectorizer.fit_transform(vocab)\n",
        "X_bag = X_bag.toarray()\n",
        "X_bag.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 6177)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiY7pL0sB91F",
        "outputId": "d4bf3d13-b55f-449b-8e42-00b33f44bcf7"
      },
      "source": [
        "model_bag = deploy_and_evaluate(X_bag , y)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1440   25]\n",
            " [  11  196]]\n",
            "\n",
            "Accuracy Score: 97.84688995215312 %\n",
            "\n",
            "Precision Score: 88.68778280542986 %\n",
            "\n",
            "Recall Score: 94.68599033816425 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv_b1fKqD7ZJ"
      },
      "source": [
        "## Now, let's form a TF-IDF model and test it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4o1a61N3DwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896d2d95-b09e-444d-c6cf-ab7ae885e22b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(vocab)\n",
        "X_tfidf.toarray()\n",
        "X_tfidf.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 6177)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUsJSVxp8WxE",
        "outputId": "512e6986-273c-49e6-ff1b-24768fcbf3df"
      },
      "source": [
        "model_tfidf = deploy_and_evaluate(X_tfidf , y)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[1465    0]\n",
            " [  48  159]]\n",
            "\n",
            "Accuracy Score: 97.1291866028708 %\n",
            "\n",
            "Precision Score: 100.0 %\n",
            "\n",
            "Recall Score: 76.81159420289855 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRcclM9nNX3r"
      },
      "source": [
        "## Custom SMS test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI2WoZW_NZrf",
        "outputId": "20afe858-46d4-40bf-8919-76b0dc6ddc1b"
      },
      "source": [
        "sms = input(\"Enter the text message: \")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text message: Congo! you won 10,000 pounds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCUEGt_0Np_d",
        "outputId": "329502b8-c6d5-4136-dd79-8c939a0e94a7"
      },
      "source": [
        "text = re.sub('[^a-zA-Z]',' ',sms)\n",
        "text = text.lower()\n",
        "text = tokenize(text)\n",
        "words = [stemmer.stem(word) for word in text if not word in stopwords.words(['english','french','spanish','german'])]\n",
        "text = ' '.join(words)\n",
        "text = [text]\n",
        "print(\"Processed text:\",text)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed text: ['congo pound']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX2ZN40sT9gH"
      },
      "source": [
        "text_bag = text\n",
        "text_tfidf = text"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_VRRaVaTQMi",
        "outputId": "bd3144d9-2106-4760-e09b-9a6801e7ffaa"
      },
      "source": [
        "text_bag = bag_vectorizer.transform(text)\n",
        "print(text_bag)\n",
        "prediction = model_bag.predict(text_bag)\n",
        "if prediction[0] == 1:\n",
        "    print(\"Spam\")\n",
        "else:\n",
        "    print(\"Harmless\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4058)\t1\n",
            "Spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3HGCrbGTlJz",
        "outputId": "66683288-8116-46b4-d1d1-5f0080219803"
      },
      "source": [
        "text_tfidf = tfidf_vectorizer.transform(text_tfidf)\n",
        "print(text_tfidf)\n",
        "prediction = model_tfidf.predict(text_tfidf)\n",
        "if prediction[0] == 1:\n",
        "    print(\"Spam\")\n",
        "else:\n",
        "    print(\"Harmless\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 4058)\t1.0\n",
            "Harmless\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EKdJZ8FVS5Q"
      },
      "source": [
        "## As is clear that the Bag of Words model is performing better than the TF-IDF model in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nRRMDTbVaYe"
      },
      "source": [
        "## **In the TF-IDF model, the recall score came low.**\n",
        "## That means that it is miss-classifying many spam messages as harmless.\n",
        "## But since the precision came 100% in TF-IDF, we can infer that no harmless message was miss-classified as spam."
      ]
    }
  ]
}